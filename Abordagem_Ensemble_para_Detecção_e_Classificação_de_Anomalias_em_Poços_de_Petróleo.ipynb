{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyPmiqlSKtvpUYFrKooyKGlM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/castrokelly/Data-Science/blob/main/Abordagem_Ensemble_para_Detec%C3%A7%C3%A3o_e_Classifica%C3%A7%C3%A3o_de_Anomalias_em_Po%C3%A7os_de_Petr%C3%B3leo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abordagem Ensemble para Detecção e Classificação de Anomalias em Poços de Petróleo: um Estudo Aplicado ao Dataset 3W\n",
        "\n",
        "**Centro de Ciências Matemáticas Aplicadas à Indústria (CeMEAI)**</br>\n",
        "**Instituto de Ciências Matemáticas e de Computação (ICMC)**</br>\n",
        "**Universidade de São Paulo**</br>\n",
        "</br>\n",
        "Aluna: **Kelly Christine Alvarenga de Castro**\n",
        "Área de concentração: Ciências de Dados\n",
        "Orientador: **Prof. Dr. Cláudio Fabiano Motta Toledo**\n",
        "</br>\n",
        "---\n",
        "\n",
        "Este Notebook apresenta o desenvolvimento de um pipeline de aprendizado de máquina para detectar e classificar anomalias em poços de petróleo utilizando o dataset 3W.\n",
        "\n",
        "A abordagem utiliza um modelo ensemble que, em primeiro lugar, decide se o estado é anômalo (classificação binária: normal vs. anômalo) e, caso seja anômalo, classifica o tipo de evento (rótulos de 1 a 9). O treinamento é realizado com dados de um poço e a validação das métricas é efetuada com dados de outro poço.\n",
        "\n"
      ],
      "metadata": {
        "id": "PTmKo-JdXZBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clonagem do repositório contendo o dataset 3W e instalação das bibliotecas necessárias:"
      ],
      "metadata": {
        "id": "iKw696Anck7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/petrobras/3W.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv4hG0T3cQQP",
        "outputId": "d7f3b4a7-bab1-478b-c6eb-1d1d9376fdb1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '3W' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras #para a integração do Keras com o scikit-learn\n",
        "!pip install PyWavelets #para a transformação wavelet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4j5zTTF6aw-A",
        "outputId": "3f1ba77e-8d67-42a8-9500-d3c21a61a3f5"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação das bibliotecas necessárias para manipulação dos dados, pré-processamento, construção e avaliação dos modelos:"
      ],
      "metadata": {
        "id": "_XnAMImmCZom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "EUTwty2mXGOC"
      },
      "outputs": [],
      "source": [
        "# Importação das Bibliotecas Necessárias\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# Para o modelo LSTM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções para Carregamento e Pré-processamento dos Dados\n",
        "\n",
        "Carregamento dos Dados\n",
        "\n",
        "Cada arquivo Parquet é carregado e uma coluna extra “instance” (identificando o arquivo) é adicionada para posterior agregação."
      ],
      "metadata": {
        "id": "caclGWVlC2BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_well_data(paths):\n",
        "    dfs = []\n",
        "    for file in paths:\n",
        "        df = pd.read_parquet(file)\n",
        "        df['instance'] = os.path.basename(file)\n",
        "        dfs.append(df)\n",
        "    return pd.concat(dfs, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "5RFgbVn1v_fr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo os caminhos dos arquivos"
      ],
      "metadata": {
        "id": "A3NsF9MD8f-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos dos arquivos para o poço 41\n",
        "paths_well41 = [\n",
        "    \"/content/3W/dataset/9/WELL-00041_20181013160201.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190814154301.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190817173916.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190819163753.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190822143519.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190906051807.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190917161232.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190924202109.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00041_20190925111412.parquet\"\n",
        "]\n",
        "\n",
        "# Caminhos dos arquivos para o poço 42\n",
        "paths_well42 = [\n",
        "    \"/content/3W/dataset/9/WELL-00042_20141217142745.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00042_20141218004109.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00042_20141218051903.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00042_20141221190219.parquet\",\n",
        "    \"/content/3W/dataset/9/WELL-00042_20141222024535.parquet\"\n",
        "]"
      ],
      "metadata": {
        "id": "zN-pbhvztaRE"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando os dados dos poços:"
      ],
      "metadata": {
        "id": "GiI2r3Yq8nBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_well41 = load_well_data(paths_well41)\n",
        "df_well42 = load_well_data(paths_well42)"
      ],
      "metadata": {
        "id": "Fs0CHlVLulUq"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição da Flag de Anomalia\n",
        "\n",
        "Cria-se a coluna \"is_anomaly\" a partir da coluna \"class\". Segundo as regras:\n",
        "\n",
        "Se \"class\" for 0, a operação é normal (is_anomaly = 0);\n",
        "\n",
        "Caso contrário (por exemplo, 9 ou 109), is_anomaly = 1."
      ],
      "metadata": {
        "id": "p0jVOMOlCs4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_well41['is_anomaly'] = df_well41['class'].apply(lambda x: 0 if x == 0 else 1)\n",
        "df_well42['is_anomaly'] = df_well42['class'].apply(lambda x: 0 if x == 0 else 1)"
      ],
      "metadata": {
        "id": "HXFNEvCbC0Wa"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregação dos Dados e Extração de Features:\n",
        "\n",
        "Para cada instância (arquivo), calcula-se as estatísticas (média, std, mínimo e máximo) das colunas de sensores. Na agregação da coluna `\"class\"`, utiliza-se o valor máximo, de forma que:\n",
        "\n",
        "* Se todos os valores forem 0, a instância é normal;\n",
        "* Se houver pelo menos um 9, o máximo será 9;\n",
        "* Se houver pelo menos um 109, o máximo será 109.\n",
        "\n",
        "Essa abordagem também é aplicada para \"is_anomaly\" (usando o .max())."
      ],
      "metadata": {
        "id": "BSNI7Qy58tA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_features(df, sensor_cols):\n",
        "    agg_funcs = ['mean', 'std', 'min', 'max']\n",
        "    agg_df = df.groupby('instance')[sensor_cols].agg(agg_funcs)\n",
        "    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n",
        "\n",
        "    # Agregação das colunas \"class\" e \"is_anomaly\" usando .max()\n",
        "    classes = df.groupby('instance')['class'].max()\n",
        "    is_anomaly = df.groupby('instance')['is_anomaly'].max()\n",
        "\n",
        "    agg_df['class'] = classes\n",
        "    agg_df['is_anomaly'] = is_anomaly\n",
        "    return agg_df"
      ],
      "metadata": {
        "id": "qZG6aiFy8zTg"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar as colunas de sensores: todas as colunas numéricas, exceto as de identificação\n",
        "cols_exclude = ['timestamp', 'instance', 'class', 'is_anomaly']\n",
        "sensor_cols = [col for col in df_well41.columns if col not in cols_exclude and pd.api.types.is_numeric_dtype(df_well41[col])]\n",
        "\n",
        "agg_well41 = aggregate_features(df_well41, sensor_cols)\n",
        "agg_well42 = aggregate_features(df_well42, sensor_cols)"
      ],
      "metadata": {
        "id": "tvAnlmI59cPQ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-Processamento: Normalização e Redução de Dimensionalidade\n",
        "\n",
        "Agora, vamos separar as features e os rótulos (usando o target binário `is_anomaly`), em seguida:\n",
        "\n",
        "* Divisão do conjunto do poço 41 em treino e validação;\n",
        "* Normalização com `StandardScaler`;\n",
        "* Redução de dimensionalidade com PCA (mantendo 95% da variância)."
      ],
      "metadata": {
        "id": "B2-L4LoU98rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_well41_clean = agg_well41.dropna(axis=1)\n",
        "agg_well42_clean = agg_well42.dropna(axis=1)"
      ],
      "metadata": {
        "id": "Sa0CKfxr-UBI"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como já temos a coluna \"class\" agregada, removemos apenas \"class\" e \"is_anomaly\" das features.\n",
        "features_well41 = agg_well41_clean.drop(['class','is_anomaly'], axis=1)\n",
        "features_well42 = agg_well42_clean.drop(['class','is_anomaly'], axis=1)\n",
        "target_well41 = agg_well41_clean['is_anomaly']\n",
        "target_well42 = agg_well42_clean['is_anomaly']\n",
        "\n",
        "selector = VarianceThreshold(threshold=0)\n",
        "features_well41_sel = selector.fit_transform(features_well41)\n",
        "features_well42_sel = selector.transform(features_well42)\n",
        "features_selected = features_well41.columns[selector.get_support()]\n",
        "\n",
        "X_well41 = pd.DataFrame(features_well41_sel, columns=features_selected, index=features_well41.index)\n",
        "X_well42 = pd.DataFrame(features_well42_sel, columns=features_selected, index=features_well42.index)\n"
      ],
      "metadata": {
        "id": "3fBcPRy7-ZdN"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão dos dados do poço 41 em treino (70%) e validação (30%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_well41, target_well41, test_size=0.3, random_state=42, stratify=target_well41\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_train_full_scaled = scaler.transform(X_well41)\n",
        "X_test_deploy_scaled = scaler.transform(X_well42)"
      ],
      "metadata": {
        "id": "TETMkz1o-dkF"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca   = pca.transform(X_val_scaled)\n",
        "X_train_full_pca = pca.transform(X_train_full_scaled)\n",
        "X_test_deploy_pca = pca.transform(X_test_deploy_scaled)\n",
        "\n",
        "n_features = X_train_pca.shape[1]\n",
        "print(\"Número de features após PCA:\", n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcAVoFPw-hJY",
        "outputId": "b6238114-aab2-47d1-c248-56be5d8056d6"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de features após PCA: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação para o Modelo LSTM\n",
        "\n",
        "Adiciona uma dimensão temporal (timestep único) para que os dados possam ser processados pelo LSTM."
      ],
      "metadata": {
        "id": "h8Xn55G2Ekm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca_lstm = X_train_pca.reshape(-1, 1, n_features)\n",
        "X_val_pca_lstm   = X_val_pca.reshape(-1, 1, n_features)\n",
        "X_train_full_pca_lstm = X_train_full_pca.reshape(-1, 1, n_features)\n",
        "X_test_deploy_pca_lstm = X_test_deploy_pca.reshape(-1, 1, n_features)"
      ],
      "metadata": {
        "id": "OwVPEreqErwM"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição dos Modelos e Criação do Ensemble\n",
        "\n",
        "São definidos três modelos:\n",
        "\n",
        "* **Random Forest** e **Gradient Boosting** para capturar relações não lineares.\n",
        "\n",
        "* **LSTM** para modelar dependências temporais, encapsulado via scikeras.\n",
        "\n",
        "Os modelos são integrados em um `ensemble via stacking` (meta-classificador: regressão logística).\n"
      ],
      "metadata": {
        "id": "AIFuQpaEExVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=input_shape, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_clf = KerasClassifier(model=create_lstm_model,\n",
        "                             model__input_shape=(1, n_features),\n",
        "                             epochs=10, batch_size=16, verbose=0)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "    ('rf', rf_clf),\n",
        "    ('gb', gb_clf),\n",
        "    ('lstm', lstm_clf)\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n"
      ],
      "metadata": {
        "id": "-GvEZ6eBFCAs"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"Class distribution in y_train:\", Counter(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWbp3PREFzKq",
        "outputId": "7c5a0b07-e29e-4c6c-c866-b08820fbed46"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in y_train: Counter({1: 6})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento e Avaliação Interna (Poço 41)\n",
        "\n",
        "Treina o ensemble com os dados de treino do poço 41 e avalia no conjunto de validação.\n",
        "\n",
        "São calculadas as métricas: acurácia, F1-Score, ROC AUC, matriz de confusão, especificidade e FAR."
      ],
      "metadata": {
        "id": "MMFQAVzbFH1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack.fit(X_train_pca, y_train)\n",
        "y_val_pred = stack.predict(X_val_pca)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, stack.predict_proba(X_val_pca)[:,1])\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "print(\"### Métricas de Validação Interna (Poço 41)\")\n",
        "print(\"Acurácia:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "print(\"Matriz de Confusão:\\n\", cm)\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "far = fp / (tn + fp) if (tn + fp) != 0 else 0\n",
        "print(\"Especificidade (SPE):\", specificity)\n",
        "print(\"Taxa de Falsos Alarmes (FAR):\", far)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "m05CxjPuFP6t",
        "outputId": "3baa994c-b5b8-49e0-bafe-71dd518cc025"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-52187f34d28c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_val_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     @available_if(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# base estimators will be used in transform, predict, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# predict_proba. They are exposed publicly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    213\u001b[0m                 delayed(_fit_single_estimator)(\n\u001b[1;32m    214\u001b[0m                     \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, fit_params, message_clsname, message)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight_is_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_encode_y\u001b[0;34m(self, y, sample_weight)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_trim_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1533\u001b[0m                 \u001b[0;34m\"y contains %d class after sample_weight \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m                 \u001b[0;34m\"trimmed classes with zero weights, while a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required."
          ]
        }
      ]
    }
  ]
}